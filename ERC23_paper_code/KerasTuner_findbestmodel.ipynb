{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from network_arch.ipynb\n",
      "importing Jupyter notebook from metrics.ipynb\n",
      "importing Jupyter notebook from plot.ipynb\n",
      "importing Jupyter notebook from settings.ipynb\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: marcodia\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import random\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import import_ipynb\n",
    "import sys\n",
    "import os \n",
    "\n",
    "import network_arch as network\n",
    "import metrics\n",
    "import plot\n",
    "import settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% >>>>> ADDITIONAL FUNCTIONS >>>>>\n",
    "def is_ndjf(month):\n",
    "    return np.logical_or(month<=2, month>=11)\n",
    "\n",
    "def is_ndjfm(month):\n",
    "    return np.logical_or(month<=3, month>=11)\n",
    "\n",
    "# MAKE THE NN ARCHITECTURE\n",
    "def make_model():\n",
    "    # Define and train the model\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = network.defineNN(HIDDENS,\n",
    "                             input1_shape = X_train.shape[1],\n",
    "                             output_shape=NLABEL,\n",
    "                             ridge_penalty1=RIDGE1,\n",
    "                             dropout=DROPOUT,\n",
    "                             act_fun='relu',\n",
    "                             network_seed=NETWORK_SEED)\n",
    "    \n",
    "    loss_function = tf.keras.losses.CategoricalCrossentropy()    \n",
    "    model.compile(\n",
    "                  optimizer = tf.keras.optimizers.Adam(learning_rate=LR_INIT),\n",
    "                  loss = loss_function,\n",
    "                  metrics = [\n",
    "                      tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\", dtype=None),\n",
    "                      metrics.PredictionAccuracy(NLABEL)\n",
    "                      ]\n",
    "                  )           \n",
    "    return model, loss_function\n",
    "\n",
    "#---------------------------------------------------\n",
    "#LEARNING RATE CALLBACK FUNCTION\n",
    "def scheduler(epoch, lr):\n",
    "    # This function keeps the initial learning rate for the first ten epochs\n",
    "    # and decreases it exponentially after that.\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import regularizers\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to pre-process data as done in NeuralNetwork_train.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code adopted from Keras online tutorial: https://keras.io/keras_tuner/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Function with Tunable Parameters\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        #units_tune = hp.Int(\"units\", min_value=32, max_value=512, step=32)   #Use this if I only want 1 layer with tunable number of nodes\n",
    "        dropout_tune = hp.Choice(\"dropout\",[0.0,.1,.2,.5,.8])                 #Numbers must be float format \n",
    "        activation_tune = hp.Choice(\"activation\", [\"relu\"])                   #To try multiple: hp.Choice(\"activation\", [\"relu\", \"linear\"])\n",
    "        l1_tune = 0.0                                   #lasso regularization\n",
    "        l2_tune = hp.Choice(\"ridge\", [0.01, 0.1,0.5,1.0,5.0,10.0,30.0])       #Numbers must be float format\n",
    "        learning_rate_tune = hp.Float(\"learning_rate\", min_value=1e-6, max_value=1e-1, sampling=\"log\")   #Moves along the logarithmic curve  \n",
    "        \n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Flatten())\n",
    "        for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
    "            model.add(\n",
    "                layers.Dense(\n",
    "                    units=hp.Int(f\"units_{i}\", min_value=32, max_value=256, step=32), #tune # of units and tune # of layers \n",
    "                    kernel_regularizer = regularizers.l1_l2(l1=l1_tune, l2=l2_tune),\n",
    "                    activation=activation_tune,\n",
    "                )\n",
    "            )\n",
    "        # Tune whether to use dropout.\n",
    "        if hp.Boolean(\"dropout\"):\n",
    "            model.add(layers.Dropout(dropout_tune))\n",
    "        model.add(layers.Dense(NLABEL, activation=\"softmax\"))   #NLABEL is the number of output classes \n",
    "\n",
    "        model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate_tune),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [32, 64, 128, 256, 512]),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    MyHyperModel(),\n",
    "    objective=\"val_accuracy\",     #use validation accuracy to choose which model is the best \n",
    "    max_trials=50,                #how many total trials to run during search \n",
    "    overwrite=True,\n",
    "    directory=ddir_out,\n",
    "    project_name= experiment_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 6\n",
      "dropout (Choice)\n",
      "{'default': 0.0, 'conditions': [], 'values': [0.0, 0.1, 0.2, 0.5, 0.8], 'ordered': True}\n",
      "activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu'], 'ordered': False}\n",
      "ridge (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 30.0], 'ordered': True}\n",
      "learning_rate (Float)\n",
      "{'default': 1e-06, 'conditions': [], 'min_value': 1e-06, 'max_value': 0.1, 'step': None, 'sampling': 'log'}\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': None}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': None}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', #monitor='val_prediction_accuracy'\n",
    "                                                   patience=25,\n",
    "                                                   mode='auto',\n",
    "                                                   restore_best_weights=True,\n",
    "                                                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 09m 59s]\n",
      "val_accuracy: 0.5397939085960388\n",
      "\n",
      "Best val_accuracy So Far: 0.5638657212257385\n",
      "Total elapsed time: 11h 14m 59s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, onehotlabels, epochs=300, validation_data=(X_val, onehotlabels_val), callbacks = [es_callback, keras.callbacks.TensorBoard(ddir_out+\"TensorBoard_output\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(tuner_results_dir):\n",
    "    \n",
    "    best_score = 0 #if score is based on val_loss, then change best_score = np.inf\n",
    "    hps = dict()\n",
    "    \n",
    "    for dirnm in os.listdir(tuner_results_dir):\n",
    "        if dirnm.startswith(\"trial\"):\n",
    "            with open(tuner_results_dir + '/' + dirnm + '/trial.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "                score = data['score']\n",
    "                if score > best_score: #switch sign if score is based on val_loss \n",
    "                    best_score = score\n",
    "                    hps = data['hyperparameters']['values']\n",
    "                     \n",
    "    return hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout': 0.0,\n",
       " 'activation': 'relu',\n",
       " 'ridge': 5.0,\n",
       " 'learning_rate': 6.656054618657257e-06,\n",
       " 'num_layers': 3,\n",
       " 'units_0': 64,\n",
       " 'units_1': 96,\n",
       " 'batch_size': 512,\n",
       " 'units_2': 96}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodel = get_best_model(ddir_out+experiment_name)\n",
    "bestmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
