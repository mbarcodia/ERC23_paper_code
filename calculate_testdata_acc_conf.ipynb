{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e922dbe-79b4-4bc1-b75a-9adc89018fb6",
   "metadata": {},
   "source": [
    "### Use Neural Networks for test data and calculate accuracy and confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8905f65b-f6a8-47e6-a48d-5f76a96350f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: marcodia\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import random\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "import import_ipynb\n",
    "import sys\n",
    "import os \n",
    "\n",
    "import network_arch as network\n",
    "import metrics\n",
    "import plot\n",
    "import settings\n",
    "import functions_misc as funcs \n",
    "\n",
    "import cftime\n",
    "import matplotlib.pyplot as plt\n",
    "import nc_time_axis\n",
    "\n",
    "from cartopy import config\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.util import add_cyclic_point\n",
    "\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from shapely.geometry.polygon import LinearRing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "662ec32c-15e0-4f3b-91ab-42004e3fb914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<< PARAMETERS >>>\n",
    "\n",
    "num_experiments = 10\n",
    "num_seeds = 10 \n",
    "exp_num = 5\n",
    "\n",
    "#confidence params\n",
    "conf_level = 'most' #OR least\n",
    "statement = 20      # or 80 \n",
    "perc = 100-statement #percentile; this number can be stated as \"the statement% conf_level confident predictions\"; ex- the 20% most confident predictions  \n",
    "subset = 'all'      # or correct, incorrect\n",
    "\n",
    "running_window_yr = 10 #years of running mean\n",
    "days_per_annualszn = 30+31+31+28 #number of days in months used; here Nov, Dec, Jan, Feb \n",
    "running_window = running_window_yr * days_per_annualszn\n",
    "\n",
    "YEARS = '1850-1949'\n",
    "STRT = pd.to_datetime('11-01-1850')\n",
    "END   = pd.to_datetime('2-28-1949')  + dt.timedelta(days=1)\n",
    "\n",
    "timeplot_full = np.arange(1850,1950)\n",
    "array_size = len(timeplot_full) - running_window_yr\n",
    "timeplot = timeplot_full[running_window_yr:(len(timeplot_full))]\n",
    "\n",
    "NLABEL = 2\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece8f3b1-f5af-4723-8118-353a7ff445a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE THE NN ARCHITECTURE\n",
    "def make_model():\n",
    "    # Define and train the model\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = network.defineNN(HIDDENS,\n",
    "                             input1_shape = X_train.shape[1],\n",
    "                             output_shape=NLABEL,\n",
    "                             ridge_penalty1=RIDGE1,\n",
    "                             dropout=DROPOUT,\n",
    "                             act_fun='relu',\n",
    "                             network_seed=NETWORK_SEED)\n",
    "    \n",
    "    loss_function = tf.keras.losses.CategoricalCrossentropy()    \n",
    "    model.compile(\n",
    "                  optimizer = tf.keras.optimizers.Adam(learning_rate=LR_INIT),\n",
    "                  loss = loss_function,\n",
    "                  metrics = [\n",
    "                      tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\", dtype=None),\n",
    "                      metrics.PredictionAccuracy(NLABEL)\n",
    "                      ]\n",
    "                  )           \n",
    "    return model, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba5df1ef-7746-4bdd-9906-1807c79ce5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_exp in np.arange(exp_num*100,exp_num*100+10):\n",
    "    EXPERIMENT = EXPERIMENT NAME #need to name from settings file\n",
    "\n",
    "    Xdata_folder = X_DATA #data folder with x-data\n",
    "    Ydata_folder = Y_DATA #data folder with y-data\n",
    "\n",
    "    ddir_X = DIRECTORY_X\n",
    "    ddir_Y = DIRECTORY_Y\n",
    "    ddir_MODEL = DIRECTORY_MODEL #directory with trained neural network model\n",
    "    ddir_out = DIRECTORY_OUT \n",
    "\n",
    "    params = settings.get_settings(EXPERIMENT)\n",
    "\n",
    "    PREDICTOR_VAR  = params['PREDICTOR_VAR']           \n",
    "    PREDICTAND_VAR = params['PREDICTAND_VAR']              \n",
    "    REGION_TOR     = params['REGION_TOR']          \n",
    "    REGION_TAND    = params['REGION_TAND']            \n",
    "    training_ens   = params['training_ens']            \n",
    "    validation_ens = params['validation_ens']           \n",
    "    testing_ens    = params['testing_ens']           \n",
    "    train_list     = params['train_list']           \n",
    "    lead           = params['lead']            \n",
    "    days_average   = params['days_average']            \n",
    "    GLOBAL_SEED    = params['GLOBAL_SEED']            \n",
    "    HIDDENS        = params['HIDDENS']          \n",
    "    DROPOUT        = params['DROPOUT']            \n",
    "    RIDGE1         = params['RIDGE1']                    \n",
    "    LR_INIT        = params['LR_INIT']\n",
    "    BATCH_SIZE     = params['BATCH_SIZE']           \n",
    "    RANDOM_SEED    = params['RANDOM_SEED']            \n",
    "    act_fun        = params['act_fun']            \n",
    "    N_EPOCHS       = params['N_EPOCHS']           \n",
    "    PATIENCE       = params['PATIENCE']   \n",
    "    \n",
    "\n",
    "#>>>>>SET UP <<<<<<<<<<<<<<<\n",
    "    np.random.seed(GLOBAL_SEED)\n",
    "    random.seed(GLOBAL_SEED)\n",
    "    tf.compat.v1.random.set_random_seed(GLOBAL_SEED)\n",
    "    \n",
    "    #Re-process training, validation to use information from them; this code is the same as NeuralNetwork_train \n",
    "    \n",
    "    time_range = xr.cftime_range(str(STRT)[:10], str(END)[:10],calendar = 'noleap') #[0:10] corresponds to full datestamp\n",
    "    time_range_ndjf = time_range.where(funcs.is_ndjf(time_range.month)).dropna()\n",
    "    TIME_X = xr.DataArray(time_range_ndjf + dt.timedelta(days=0), dims=['time'])     \n",
    "    TIME_Y = xr.DataArray(time_range_ndjf + dt.timedelta(days=lead+days_average), dims=['time'])  \n",
    "    \n",
    "    # ----- X TRAINING ------\n",
    "    X_finame = PREDICTOR_VAR+'_'+REGION_TOR+'_'+YEARS+'_'+'ens'+train_list+'_dailyanom_detrend.nc'\n",
    "    X_all_full = xr.open_dataarray(ddir_X+X_finame)\n",
    "    X_all = X_all_full.where(X_all_full.time == TIME_X, drop=True)\n",
    "\n",
    "\n",
    "    Xtrain = X_all.stack(time_all=('ens','time')) # lat,lon,time*8 (8= number of training ens members) \n",
    "    Xtrain = Xtrain.transpose('time_all','lat','lon') # time*8,lat,lon\n",
    "\n",
    "    Xtrain_std = np.std(Xtrain,axis=0)\n",
    "    Xtrain_mean = np.mean(Xtrain,axis=0)\n",
    "    Xtrain = (Xtrain-Xtrain_mean)/Xtrain_std\n",
    "    X_train = Xtrain.stack(z=('lat','lon'))\n",
    "\n",
    "    # ---------- X VALIDATION (and TESTING)----------\n",
    "    X_finame  = PREDICTOR_VAR+'_'+REGION_TOR+'_'+YEARS+'_'+'ens'+str(validation_ens)+'_dailyanom_detrend.nc'\n",
    "    Xval = xr.open_dataarray(ddir_X+X_finame)\n",
    "\n",
    "    Xval= Xval.where(Xval.time == TIME_X, drop=True)\n",
    "    Xval = (Xval - Xtrain_mean)/Xtrain_std\n",
    "    print(Xval.time)\n",
    "    \n",
    "    X_TEST_finame  = PREDICTOR_VAR+'_'+REGION_TOR+'_'+YEARS+'_'+'ens'+str(testing_ens)+'_dailyanom_detrend.nc'\n",
    "    Xtest = xr.open_dataarray(ddir_X+X_TEST_finame)\n",
    "\n",
    "    Xtest = Xtest.where(Xtest.time == TIME_X, drop=True)\n",
    "    Xtest = (Xtest - Xtrain_mean)/Xtrain_std\n",
    "\n",
    "    #%% ----- Y TRAINING--------\n",
    "\n",
    "    Ytrain_finame = PREDICTAND_VAR+'_'+REGION_TAND+'_boxavg_'+YEARS+'_'+'ens'+train_list+'_dailyanom_detrend_14dayavg.nc'\n",
    "    Y_all = xr.open_dataarray(ddir_Y+Ytrain_finame)\n",
    "    Y_all= Y_all.where(Y_all.time == TIME_Y, drop=True)\n",
    "\n",
    "    # ----- Standardize Y training -----\n",
    "    Ytrain = Y_all[:,:]    #already box averaged so no lat or lon dimension \n",
    "    Ytrain = Ytrain.stack(time_all=('ens','time')) # time*8 (Ytrain time is actually 365*100-(13*8) because of 14-day average)\n",
    "    Ytrain_med = np.median(Ytrain)\n",
    "    Ytrain = Ytrain - Ytrain_med       #Subtracting the median forces the output above or below zero \n",
    "\n",
    "    # ----- Y VALIDATION --------\n",
    "    Yval_finame = PREDICTAND_VAR+'_'+REGION_TAND+'_boxavg_'+YEARS+'_'+'ens'+str(validation_ens)+'_dailyanom_detrend_14dayavg.nc'\n",
    "    Yval_all  = xr.open_dataarray(ddir_Y+Yval_finame)\n",
    "    Yval_all= Yval_all.where(Yval_all.time == TIME_Y, drop=True)\n",
    "\n",
    "    # ----- Standardize Y validation -----\n",
    "    Yval = Yval_all[:]\n",
    "    Yval = Yval - Ytrain_med         \n",
    "    print(Yval.time)\n",
    "    # ----- Grab Y testing -----\n",
    "\n",
    "    Ytest_finame = PREDICTAND_VAR+'_'+REGION_TAND+'_boxavg_'+YEARS+'_'+'ens'+str(testing_ens)+'_dailyanom_detrend_14dayavg.nc'\n",
    "    Ytest_all  = xr.open_dataarray(ddir_Y+Ytest_finame)\n",
    "    Ytest_all= Ytest_all.where(Ytest_all.time == TIME_Y, drop=True)\n",
    "\n",
    "    Ytest = Ytest_all[:]\n",
    "    Ytest = Ytest - Ytrain_med \n",
    "\n",
    "    # ----- Make binary -----\n",
    "    # training\n",
    "    Ytrain[np.where(Ytrain>=0)[0]] = 1\n",
    "    Ytrain[np.where(Ytrain<0)[0]]  = 0\n",
    "    # validation\n",
    "    Yval[np.where(Yval>=0)[0]] = 1\n",
    "    Yval[np.where(Yval<0)[0]]  = 0\n",
    "    # testing\n",
    "    Ytest[np.where(Ytest>=0)[0]] = 1\n",
    "    Ytest[np.where(Ytest<0)[0]]  = 0\n",
    "\n",
    "    # make Yval have equal 0s and 1s so that random chance is 50%\n",
    "    n_valzero = np.shape(np.where(Yval==0)[0])[0]\n",
    "    n_valone  = np.shape(np.where(Yval==1)[0])[0]\n",
    "    i_valzero = np.where(Yval==0)[0]\n",
    "    i_valone  = np.where(Yval==1)[0]\n",
    "\n",
    "    if n_valone > n_valzero:\n",
    "        isubset_valone = np.random.choice(i_valone,size=n_valzero,replace=False)\n",
    "        i_valnew = np.sort(np.append(i_valzero,isubset_valone))\n",
    "        Yval = Yval.isel(time=i_valnew,drop=True)\n",
    "        X_val  = Xval[i_valnew].stack(z=('lat','lon'))\n",
    "    elif n_valone < n_valzero:\n",
    "        isubset_valzero = np.random.choice(i_valzero,size=n_valone,replace=False)\n",
    "        i_valnew = np.sort(np.append(isubset_valzero,i_valone))\n",
    "        Yval = Yval.isel(time=i_valnew,drop=True)\n",
    "        X_val  = Xval[i_valnew].stack(z=('lat','lon'))\n",
    "    else:\n",
    "        X_val = Xval.stack(z=('lat','lon'))\n",
    "\n",
    "    # ----- Make one hot vector -----\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    onehotlabels      = enc.fit_transform(np.array(Ytrain).reshape(-1, 1)).toarray()\n",
    "    onehotlabels_val  = enc.fit_transform(np.array(Yval).reshape(-1, 1)).toarray()\n",
    "    onehotlabels_test  = enc.fit_transform(np.array(Ytest).reshape(-1, 1)).toarray()\n",
    "\n",
    "    X_test_calcs = Xtest.stack(z=('lat','lon'))\n",
    "    hotlabels_test_calcs = onehotlabels_test[:,:NLABEL]\n",
    "\n",
    "    output_class_test = np.array(Ytest)\n",
    "    output_test = (output_class_test.reshape(-1,1) == np.unique(output_class_test)).astype(int)\n",
    "    calcpercent = lambda cat: str((np.sum(output_class_test == cat)/len(output_class_test)*100).astype(int))\n",
    "    # Print out the sizes of each class\n",
    "    print('Frequency for each Category')\n",
    "    print('Below Zero: ' + calcpercent(0) + '%')\n",
    "    print('Above Zero: ' + calcpercent(1) + '%')\n",
    "     \n",
    "    #>>>>>>>>Analyze test data on trained model<<<<<<<<\n",
    "    NETWORK_SEED=0\n",
    "    model, LOSS = make_model() \n",
    "\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "    \n",
    "    \n",
    "    #Need to redefine empty matrices for each experiment\n",
    "    #Code modified from Dr. Kirsten Mayer\n",
    "    \n",
    "    accuracy_test = np.zeros([array_size])\n",
    "    stored_accuracy = np.zeros(len(RANDOM_SEED))\n",
    "    ind = 0\n",
    "    ind2 = 0\n",
    "    acc_test = np.zeros(shape=(10,20)) + np.nan    #this is the shape of [# seeds, # of confidence intervals]\n",
    "    acc_val = np.zeros(shape=(10,20)) + np.nan\n",
    "    acc_moving = np.zeros(shape=(num_seeds,array_size)) + np.nan\n",
    "    \n",
    "    #<<<<< LOAD MODEL TO RUN CALCULATIONS ON PREDICTIONS >>>>>>>\n",
    "    for SEED in RANDOM_SEED:\n",
    "        if ind < num_experiments:\n",
    "            model.load_weights(ddir_MODEL+'_operationalseed'+str(SEED)+'.h5')\n",
    "            X1= X_test_calcs.copy(deep=True)\n",
    "            \n",
    "            #<<<Apply testing data to trained model>>>>>>>>>>>>> \n",
    "            Ptest = model.predict(X_test_calcs)    #predicted values on test data - softmax output of confidence \n",
    "            Cptest_pred = Ptest.argmax(axis=1)     #0,1 of predicted  \n",
    "            Cttest_true = hotlabels_test_calcs.argmax(axis=1) #true values on validation \n",
    "                  \n",
    "            conf_pred_val = model.predict(X_val)           # softmax output\n",
    "            cat_pred_val  = np.argmax(conf_pred_val, axis = 1) # categorical output\n",
    "            max_conf_val  = np.max(conf_pred_val, axis = 1)   # predicted category confidence\n",
    "          \n",
    "            results_test = model.evaluate(X_test_calcs,hotlabels_test_calcs,verbose = 2) #prints out model evaluation \n",
    "            stored_accuracy[ind] = results_test[2] #2 hardcoded to get the accuracy (categorical and prediction accuracy are the same here)\n",
    "\n",
    "        # ----- EVALUATE MODEL -----  \n",
    "            conf_pred_test = model.predict(X_test_calcs)           # softmax output\n",
    "            cat_pred_test  = np.argmax(conf_pred_test, axis = 1) # categorical output\n",
    "            max_conf_test  = np.max(conf_pred_test, axis = 1)   # predicted category confidence\n",
    "            \n",
    "            #<<<< Rank data into most confident predictions for testing and validation>>>>>>>\n",
    "            \n",
    "            for p,per in enumerate(np.arange(0,100,5)):\n",
    "                if ind < num_seeds: \n",
    "                    i_cover_test = np.where(max_conf_test >= np.percentile(max_conf_test,per))[0]\n",
    "                    icorr_test   = np.where(cat_pred_test[i_cover_test] == Ytest[i_cover_test])[0]\n",
    "                    if len(i_cover_test) == 0:\n",
    "                        acc_test[ind,p] = 0.\n",
    "                    else:\n",
    "                        acc_test[ind,p] = (len(icorr_test)/len(i_cover_test)) * 100   \n",
    "        \n",
    "        \n",
    "            for p,per in enumerate(np.arange(0,100,5)):\n",
    "                if ind < num_seeds: \n",
    "                    i_cover_val = np.where(max_conf_val >= np.percentile(max_conf_val,per))[0]\n",
    "                    icorr_val   = np.where(cat_pred_val[i_cover_val] == Yval[i_cover_val])[0]\n",
    "                    if len(i_cover_val) == 0:\n",
    "                        acc_val[ind,p] = 0.\n",
    "                    else:\n",
    "                        acc_val[ind,p] = (len(icorr_val)/len(i_cover_val)) * 100\n",
    "                \n",
    "            #<<<<<<<<<<>>>>>>>>>>>>>\n",
    "            \n",
    "            #<<<<<<<<<<Take window of accuracy>>>>>>>>>>>\n",
    "            for i in np.arange(0,len(Cptest_pred)-running_window_yr,(days_per_annualszn)):\n",
    "                if (running_window+i) <= (len(Cptest_pred)):\n",
    "                    modelcorr_test = Cptest_pred[i:running_window+i]==Cttest_true[i:running_window+i]\n",
    "                    nmodelcorr_test = modelcorr_test[modelcorr_test].shape[0]\n",
    "                    ntest_test = Cttest_true[i:running_window+i].shape[0]\n",
    "                    index_test = int(i/days_per_annualszn)\n",
    "                    if index_test<99:\n",
    "                        accuracy_test[index_test] = 100*nmodelcorr_test/ntest_test\n",
    "                        \n",
    "            accuracy_test_xr = xr.DataArray(accuracy_test)\n",
    "            filename = 'accuracy_testdata_'+str(running_window_yr)+'yr_runavg_exp'+EXPERIMENT[-3:]+'_seed'+str(SEED)+'.nc'\n",
    "            #accuracy_test_xr.to_netcdf(ddir_out+filename, mode='w')\n",
    "            \n",
    "            #<<<<<<<<<<>>>>>>>>>>>>>\n",
    "            \n",
    "            #Calculate Accuracy for confident predictions \n",
    "            \n",
    "            for i in np.arange(0,len(Cptest_pred)-running_window_yr,(days_per_annualszn)):\n",
    "                 if (running_window+i) <= (len(Cptest_pred)):\n",
    "            \n",
    "                    max_conf  = np.max(Ptest[i:running_window+i], axis = 1)   # predicted category confidence\n",
    "                    Cptest_pred_cut = Cptest_pred[i:running_window+i]\n",
    "                    Ytest_cut = Ytest[i:running_window+i]\n",
    "                    \n",
    "                    if conf_level == 'most':\n",
    "                        i_cover = np.where(max_conf >= np.percentile(max_conf,perc))[0]  #find X% most confident predictions \n",
    "                    if conf_level == 'least':\n",
    "                        i_cover = np.where(max_conf <= np.percentile(max_conf,perc))[0]  #find X% least confident predictions \n",
    "                        \n",
    "                    icorr   = np.where(Cptest_pred_cut[i_cover] == Ytest_cut[i_cover])[0]   #find where the confident predictions match the truth values (ie find correct); Ytest is the NOT onehotencoded Y-values \n",
    "\n",
    "                    index_test = int(i/days_per_annualszn)\n",
    "                    \n",
    "                    if index_test<99:\n",
    "                        acc_moving[ind,index_test] = (len(icorr)/len(i_cover)) * 100                #calculate accuracy; ind is the network seed  \n",
    "            \n",
    "            \n",
    "            acc_vs_conf_moving = xr.DataArray(acc_moving)\n",
    "            filename_moving = str(statement)+'confidence_vs_accuracy_'+str(running_window_yr)+'yr_runavg_'+EXPERIMENT[-3:]+'_allseeds.nc'\n",
    "            acc_vs_conf_moving.to_netcdf(ddir_out+filename_moving, mode='w',format='NETCDF4')\n",
    "            \n",
    "            #<<<<<<<<<<<Split most confident data into correct and incorrect>>>>>>>\n",
    "            #softmax:\n",
    "            \n",
    "            max_logits = np.max(Ptest,axis=-1)\n",
    "            if conf_level == 'most':\n",
    "                i_cover = np.where(max_logits >= np.percentile(max_logits, perc))[0]\n",
    "                \n",
    "            if conf_level == 'least':\n",
    "                i_cover = np.where(max_logits < np.percentile(max_logits, perc))[0]\n",
    "                \n",
    "                \n",
    "            cat_true_conf = Cttest_true[i_cover]\n",
    "            cat_pred_conf = Cptest_pred[i_cover]\n",
    "            \n",
    "            if subset == 'correct':\n",
    "                #location of correct predictions\n",
    "                icorr = np.where(Cptest_pred[i_cover] - Cttest_true[i_cover] == 0)[0]\n",
    "                X1_subset = X1[i_cover][icorr] #index Xtest with indicies of CONFIDENT & CORRECT predictions; for output compositing, replace X1 with Y variable \n",
    "                X_subset_time = X1.time[i_cover][icorr] #index with indicies of CONFIDENT & CORRECT predictions\n",
    "                Y_true_subset = cat_true_conf[icorr]\n",
    "                Y_pred_subset = cat_pred_conf[icorr]\n",
    "                time = X_test_calcs.time[icorr]\n",
    "            \n",
    "            if subset == 'incorrect': #confident but incorrect/wrong prediction\n",
    "                #location of incorrect predictions\n",
    "                i_incorr = np.where(Cptest_pred[i_cover] - Cttest_true[i_cover] != 0)[0]\n",
    "                X1_subset = X1[i_cover][i_incorr] #index Xtest with indicies of INCORRECT predictions\n",
    "                X_subset_time = X1.time[i_cover][i_incorr] #index with indicies of INCORRECT predictions\n",
    "                Y_true_subset = cat_true_conf[i_incorr]\n",
    "                Y_pred_subset = cat_pred_conf[i_incorr]\n",
    "                time = X_test_calcs.time[i_incorr]\n",
    "            \n",
    "            if subset == 'all':    #all confident predictions (correct and incorrect) \n",
    "                i_incorr = np.where(Cptest_pred[i_cover] - Cttest_true[i_cover] <= 1)[0]\n",
    "                X1_subset = X1[i_cover][i_incorr] #index Xtest with indicies of ALL predictions\n",
    "                X_subset_time = X1.time[i_cover][i_incorr] #index with indicies of ALL predictions\n",
    "                Y_true_subset = cat_true_conf[i_incorr]\n",
    "                Y_pred_subset = cat_pred_conf[i_incorr]\n",
    "                time = X_test_calcs.time[i_incorr]\n",
    "            \n",
    "            # >>>>> LOOP THROUGH EACH CLASS: Calc & DATA MAPS FOR THE CLASS >>>>>\n",
    "            classes = np.arange(0,NLABEL)\n",
    "            for c in classes:\n",
    "                sample = X1_subset.copy()\n",
    "                icat = np.where(Y_true_subset == c)[0]# will be the same category as predicted if subset = 'correct'\n",
    "                time_class = time[icat]\n",
    "                Data1 = X1_subset[icat]\n",
    "                Data1map = Data1[:,:np.shape(X1_subset)[1]].unstack('z')\n",
    "\n",
    "                # <<<<< Array prep for plotting and saving to netcdf <<<<<\n",
    "\n",
    "                # >>>>> Convert from numpy to xarray to save >>>>>\n",
    "                Data1map_xr = xr.DataArray(Data1map) #,           #this retains the time dimension; changes because # of samples changes \n",
    "  \n",
    "                # <<<<< Convert from numpy to xarray to save <<<<<\n",
    "                Data_mean = np.nanmean(Data1map_xr,axis=0)   #this is a map to composite; ie averaged over time \n",
    "                Data_mean_xr = xr.DataArray(Data_mean)\n",
    "                Data1map_xr.to_netcdf(ddir_out+subset+'_'+str(statement)+'perc_'+conf_level+'confident_predictions_'+EXPERIMENT[-3:]+'_seed'+str(SEED)+'_class'+str(c)+'.nc')\n",
    "                Data_mean_xr.to_netcdf(ddir_out+subset+'TIMEMEAN_'+str(statement)+'perc_'+conf_level+'confident_predictions_'+EXPERIMENT[-3:]+'_seed'+str(SEED)+'_class'+str(c)+'.nc')\n",
    "        \n",
    "        ind = ind+1 \n",
    "        ind2 = ind2+1\n",
    "\n",
    "        acc_vs_conf_test = xr.DataArray(acc_test)\n",
    "        filename_test = 'confidence_vs_accuracy_TESTING_'+EXPERIMENT[-3:]+'_allseeds.nc'\n",
    "        acc_vs_conf_test.to_netcdf(ddir_out+filename_test, mode='w')  \n",
    "        \n",
    "        acc_vs_conf_val = xr.DataArray(acc_val)\n",
    "        filename_val = 'confidence_vs_accuracy_VALIDATION_'+EXPERIMENT[-3:]+'_allseeds.nc'\n",
    "        acc_vs_conf_val.to_netcdf(ddir_out+filename_val, mode='w')\n",
    "        \n",
    "        stored_accuracy_xr = xr.DataArray(stored_accuracy)\n",
    "        filename2 = 'overall_accuracy_testdata'+'_exp'+EXPERIMENT[-3:]+'_allseeds'+'.nc'\n",
    "        stored_accuracy_xr.to_netcdf(ddir_out+filename2, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17146f8-988f-4618-bfac-b64b41d3d93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1066e5-f8b2-4af8-8a32-804877667c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
