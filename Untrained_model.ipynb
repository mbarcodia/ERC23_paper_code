{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: marcodia\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import random\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "import import_ipynb\n",
    "import sys\n",
    "import os \n",
    "\n",
    "!ln -s /Users/marcodia/Research/Kirsten_code/network_arch.ipynb network_arch.ipynb\n",
    "!ln -s /Users/marcodia/Research/Kirsten_code/metrics.ipynb metrics.ipynb\n",
    "!ln -s /Users/marcodia/Research/Kirsten_code/plot.ipynb plot.ipynb\n",
    "!ln -s /Users/marcodia/Research/Kirsten_code/settings.ipynb settings.ipynb\n",
    "\n",
    "import network_arch as network\n",
    "import metrics\n",
    "import plot\n",
    "import settings\n",
    "\n",
    "import cftime\n",
    "import matplotlib.pyplot as plt\n",
    "import nc_time_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% >>>>> ADDITIONAL FUNCTIONS >>>>>\n",
    "def is_ndjf(month):\n",
    "    return np.logical_or(month<=2, month>=11)\n",
    "\n",
    "def is_ndjfm(month):\n",
    "    return np.logical_or(month<=3, month>=11)\n",
    "\n",
    "#%% >>>>> NETWORK SETUP >>>>>\n",
    "# MAKE THE NN ARCHITECTURE\n",
    "def make_model():\n",
    "    # Define and train the model\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # NORMAL ANN\n",
    "    model = network.defineNN(HIDDENS,\n",
    "                             input1_shape = X_train.shape[1],\n",
    "                             output_shape=NLABEL,\n",
    "                             ridge_penalty1=RIDGE1,\n",
    "                             dropout=DROPOUT,\n",
    "                             act_fun='relu',\n",
    "                             network_seed=NETWORK_SEED)\n",
    "    \n",
    "    loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "    #loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    \n",
    "    model.compile(\n",
    "                  optimizer = tf.keras.optimizers.Adam(learning_rate=LR_INIT),\n",
    "                  loss = loss_function,\n",
    "                  metrics = [\n",
    "                      tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\", dtype=None),\n",
    "                      metrics.PredictionAccuracy(NLABEL)\n",
    "                      # in the future, remove metrics.PredictionAccuracy(NLABEL) from above, \n",
    "                      # & use \"val_categorical_accuracy\" (not \"val_prediction_accuracy\")\n",
    "                      # in early stopping code (below) and plot.py code\n",
    "                      # ...then you dont need to import metrics!\n",
    "                      # \"metrics.PredictionAccuracy(NLABEL)\" was left over\n",
    "                      # from abstention code since it removes those that were abstained on\n",
    "                      # to calculate accuracy.\n",
    "                      ]\n",
    "                  )           \n",
    "    \n",
    "    #model.summary()    \n",
    "    return model, loss_function\n",
    "\n",
    "#---------------------------------------------------\n",
    "#LEARNING RATE CALLBACK FUNCTION\n",
    "def scheduler(epoch, lr):\n",
    "    # This function keeps the initial learning rate for the first ten epochs\n",
    "    # and decreases it exponentially after that.\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "#---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 10\n",
    "num_seeds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Pull Parameters from settings.ipynb ------\n",
    "for sub_exp in np.arange(101,102):\n",
    "    EXPERIMENT = 'exp_1/exp_'+str(sub_exp)\n",
    "\n",
    "    Xdata_folder = 'exp_1/exp_101'\n",
    "    Ydata_folder = 'exp_1/exp_101'\n",
    "\n",
    "    ddir_X = '/Users/marcodia/Research/Data/processed_fields/Week_34/'+Xdata_folder+'/'\n",
    "    ddir_Y = '/Users/marcodia/Research/Data/processed_fields/Week_34/'+Ydata_folder+'/'\n",
    "\n",
    "    #ddir = '/Users/marcodia/Research/Data/processed_fields/'+EXPERIMENT+'/'\n",
    "    ddir_out = '/Users/marcodia/Research/Data/processed_fields/Week_45/exp_1/untrained_model/'\n",
    "    params = settings.get_settings(EXPERIMENT)\n",
    "\n",
    "    PREDICTOR_VAR  = params['PREDICTOR_VAR']           \n",
    "    PREDICTAND_VAR = params['PREDICTAND_VAR']              \n",
    "    REGION_TOR     = params['REGION_TOR']          \n",
    "    REGION_TAND    = params['REGION_TAND']            \n",
    "    training_ens   = params['training_ens']            \n",
    "    validation_ens = params['validation_ens']           \n",
    "    testing_ens    = params['testing_ens']           \n",
    "    train_list     = params['train_list']           \n",
    "    lead           = 21 #params['lead']            \n",
    "    days_average   = params['days_average']            \n",
    "    GLOBAL_SEED    = params['GLOBAL_SEED']            \n",
    "    HIDDENS        = params['HIDDENS']          \n",
    "    DROPOUT        = params['DROPOUT']            \n",
    "    RIDGE1         = params['RIDGE1']                    \n",
    "    LR_INIT        = params['LR_INIT']\n",
    "    BATCH_SIZE     = params['BATCH_SIZE']           \n",
    "    RANDOM_SEED    = params['RANDOM_SEED']            \n",
    "    act_fun        = params['act_fun']            \n",
    "    N_EPOCHS       = params['N_EPOCHS']           \n",
    "    PATIENCE       = params['PATIENCE']   \n",
    "    \n",
    "#>>>>>SET UP <<<<<<<<<<<<<<<\n",
    "    np.random.seed(GLOBAL_SEED)\n",
    "    random.seed(GLOBAL_SEED)\n",
    "    tf.compat.v1.random.set_random_seed(GLOBAL_SEED)\n",
    "\n",
    "    NLABEL = 2\n",
    "\n",
    "    YEARS = '1850-1949'\n",
    "    STRT = pd.to_datetime('11-01-1850')\n",
    "    END   = pd.to_datetime('2-28-1949')  + dt.timedelta(days=1)\n",
    "\n",
    "    time_range = xr.cftime_range(str(STRT)[:10], str(END)[:10],calendar = 'noleap') #[0:10] corresponds to full datestamp\n",
    "    time_range_ndjf = time_range.where(is_ndjf(time_range.month)).dropna()\n",
    "    TIME_X = xr.DataArray(time_range_ndjf + dt.timedelta(days=0), dims=['time'])     \n",
    "    TIME_Y = xr.DataArray(time_range_ndjf + dt.timedelta(days=lead+days_average), dims=['time'])  #below comment explains time segmentation\n",
    "\n",
    "        #Based on the 14-day running average, Dec 6 1850 corresponds to the prior 14days averaged together. \n",
    "        #So a Nov 1 1850 prediction is for 21 days later of the 3-4week future average\n",
    "    \n",
    "    # ----- X TRAINING ------\n",
    "    X_finame = PREDICTOR_VAR+'_'+REGION_TOR+'_'+YEARS+'_'+'ens'+train_list+'_dailyanom_detrend.nc'\n",
    "    X_all_full = xr.open_dataarray(ddir_X+X_finame)\n",
    "    X_all = X_all_full.where(X_all_full.time == TIME_X, drop=True)\n",
    "\n",
    "\n",
    "    Xtrain = X_all.stack(time_all=('ens','time')) # lat,lon,time*8 (8= number of training ens members) \n",
    "    Xtrain = Xtrain.transpose('time_all','lat','lon') # time*8,lat,lon\n",
    "\n",
    "    Xtrain_std = np.std(Xtrain,axis=0)\n",
    "    Xtrain_mean = np.mean(Xtrain,axis=0)\n",
    "    Xtrain = (Xtrain-Xtrain_mean)/Xtrain_std\n",
    "    X_train = Xtrain.stack(z=('lat','lon'))\n",
    "\n",
    "    # ---------- X VALIDATION (and TESTING)----------\n",
    "    X_finame  = PREDICTOR_VAR+'_'+REGION_TOR+'_'+YEARS+'_'+'ens'+str(validation_ens)+'_dailyanom_detrend.nc'\n",
    "    Xval = xr.open_dataarray(ddir_X+X_finame)\n",
    "\n",
    "    Xval= Xval.where(Xval.time == TIME_X, drop=True)\n",
    "    Xval = (Xval - Xtrain_mean)/Xtrain_std\n",
    "\n",
    "    X_TEST_finame  = PREDICTOR_VAR+'_'+REGION_TOR+'_'+YEARS+'_'+'ens'+str(testing_ens)+'_dailyanom_detrend.nc'\n",
    "    Xtest = xr.open_dataarray(ddir_X+X_TEST_finame)\n",
    "\n",
    "    Xtest = Xtest.where(Xtest.time == TIME_X, drop=True)\n",
    "    Xtest = (Xtest - Xtrain_mean)/Xtrain_std\n",
    "\n",
    "    #%% ----- Y TRAINING--------\n",
    "\n",
    "    Ytrain_finame = PREDICTAND_VAR+'_'+REGION_TAND+'_boxavg_'+YEARS+'_'+'ens'+train_list+'_dailyanom_detrend_14dayavg.nc'\n",
    "    Y_all = xr.open_dataarray(ddir_Y+Ytrain_finame)\n",
    "    Y_all= Y_all.where(Y_all.time == TIME_Y, drop=True)\n",
    "\n",
    "    # ----- Standardize Y training -----\n",
    "    Ytrain = Y_all[:,:]    #already box averaged so no lat or lon dimension \n",
    "    Ytrain = Ytrain.stack(time_all=('ens','time')) # time*8 (Ytrain time is actually 365*100-(13*8) because of 14-day average)\n",
    "    Ytrain_med = np.median(Ytrain)\n",
    "    Ytrain = Ytrain - Ytrain_med       #Subtracting the median forces the output above or below zero \n",
    "\n",
    "    # ----- Y VALIDATION --------\n",
    "    Yval_finame = PREDICTAND_VAR+'_'+REGION_TAND+'_boxavg_'+YEARS+'_'+'ens'+str(validation_ens)+'_dailyanom_detrend_14dayavg.nc'\n",
    "    Yval_all  = xr.open_dataarray(ddir_Y+Yval_finame)\n",
    "    Yval_all= Yval_all.where(Yval_all.time == TIME_Y, drop=True)\n",
    "\n",
    "    # ----- Standardize Y validation -----\n",
    "    Yval = Yval_all[:]\n",
    "    Yval = Yval - Ytrain_med         \n",
    "\n",
    "    # ----- Grab Y testing -----\n",
    "\n",
    "    Ytest_finame = PREDICTAND_VAR+'_'+REGION_TAND+'_boxavg_'+YEARS+'_'+'ens'+str(testing_ens)+'_dailyanom_detrend_14dayavg.nc'\n",
    "    Ytest_all  = xr.open_dataarray(ddir_Y+Ytest_finame)\n",
    "    Ytest_all= Ytest_all.where(Ytest_all.time == TIME_Y, drop=True)\n",
    "\n",
    "    Ytest = Ytest_all[:]\n",
    "    Ytest = Ytest - Ytrain_med \n",
    "\n",
    "    # ----- Make binary -----\n",
    "    # training\n",
    "    Ytrain[np.where(Ytrain>=0)[0]] = 1\n",
    "    Ytrain[np.where(Ytrain<0)[0]]  = 0\n",
    "    # validation\n",
    "    Yval[np.where(Yval>=0)[0]] = 1\n",
    "    Yval[np.where(Yval<0)[0]]  = 0\n",
    "    # testing\n",
    "    Ytest[np.where(Ytest>=0)[0]] = 1\n",
    "    Ytest[np.where(Ytest<0)[0]]  = 0\n",
    "\n",
    "    # make Yval have equal 0s and 1s so that random chance is 50%\n",
    "    n_valzero = np.shape(np.where(Yval==0)[0])[0]\n",
    "    n_valone  = np.shape(np.where(Yval==1)[0])[0]\n",
    "    i_valzero = np.where(Yval==0)[0]\n",
    "    i_valone  = np.where(Yval==1)[0]\n",
    "\n",
    "    if n_valone > n_valzero:\n",
    "        isubset_valone = np.random.choice(i_valone,size=n_valzero,replace=False)\n",
    "        i_valnew = np.sort(np.append(i_valzero,isubset_valone))\n",
    "        Yval = Yval.isel(time=i_valnew,drop=True)\n",
    "        X_val  = Xval[i_valnew].stack(z=('lat','lon'))\n",
    "    elif n_valone < n_valzero:\n",
    "        isubset_valzero = np.random.choice(i_valzero,size=n_valone,replace=False)\n",
    "        i_valnew = np.sort(np.append(isubset_valzero,i_valone))\n",
    "        Yval = Yval.isel(time=i_valnew,drop=True)\n",
    "        X_val  = Xval[i_valnew].stack(z=('lat','lon'))\n",
    "    else:\n",
    "        X_val = Xval.stack(z=('lat','lon'))\n",
    "\n",
    "    # ----- Make one hot vector -----\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    onehotlabels      = enc.fit_transform(np.array(Ytrain).reshape(-1, 1)).toarray()\n",
    "    onehotlabels_val  = enc.fit_transform(np.array(Yval).reshape(-1, 1)).toarray()\n",
    "    onehotlabels_test  = enc.fit_transform(np.array(Ytest).reshape(-1, 1)).toarray()\n",
    "\n",
    "    X_test_calcs = Xtest.stack(z=('lat','lon'))\n",
    "    hotlabels_test_calcs = onehotlabels_test[:,:NLABEL]\n",
    "\n",
    "# DO NOT TRAIN MODEL; we want to see how the model looks with NO training, so just predicting on testing data \n",
    "# there is no early stopping callback and no plotting of loss/accuracy because of history variable in plotting code \n",
    "\n",
    "    for NETWORK_SEED in RANDOM_SEED:\n",
    "        print(NETWORK_SEED)\n",
    "        # the network seed changes the random seed for the initialized weights.\n",
    "        # this means that a different network seed can give a different result (e.g. it finds a different minimum in the loss)\n",
    "        # ----- MAKE NN -----\n",
    "        es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',     #monitor='val_prediction_accuracy'\n",
    "                                                       patience=PATIENCE,\n",
    "                                                       mode='auto',\n",
    "                                                       restore_best_weights=True,\n",
    "                                                       verbose=1)\n",
    "        #lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose=0)\n",
    "        #callbacks = [es_callback,lr_callback]\n",
    "        #callbacks = [es_callback]\n",
    "        callbacks = []\n",
    "\n",
    "        model, loss_function = make_model()\n",
    "\n",
    "        hotlabels = onehotlabels[:,:model.output_shape[-1]]\n",
    "        hotlabels_val = onehotlabels_val[:,:model.output_shape[-1]]\n",
    "\n",
    "        # ----- TRAINING NETWORK -----\n",
    "        start_time = time.time()\n",
    "        # history = model.fit(X_train,\n",
    "        #                     hotlabels,\n",
    "        #                     validation_data=(X_val, hotlabels_val),\n",
    "        #                     batch_size=BATCH_SIZE,\n",
    "        #                     epochs=N_EPOCHS,\n",
    "        #                     shuffle=True,\n",
    "        #                     verbose=0,\n",
    "        #                     callbacks=callbacks,\n",
    "        #                    )\n",
    "\n",
    "        history = model.predict(X_test_calcs)\n",
    "        stop_time = time.time()\n",
    "        tf.print(f\"Elapsed time during fit = {stop_time - start_time:.2f} seconds\\n\")\n",
    "\n",
    "        # ----- SAVE MODEL -----\n",
    "        fi = EXPERIMENT[-3:]+'_operationalseed'+str(NETWORK_SEED)+'.h5' \n",
    "        model.save_weights(ddir_out+fi)\n",
    "\n",
    "        # ----- PLOT THE RESULTS -----\n",
    "        # plot.plot_results(\n",
    "        #     history,\n",
    "        #     exp_info=(N_EPOCHS, HIDDENS, LR_INIT, BATCH_SIZE, NETWORK_SEED, PATIENCE, RIDGE1, DROPOUT),\n",
    "        #     showplot=True\n",
    "        # )\n",
    "\n",
    "        # ----- PRINT THE RESULTS -----\n",
    "        predictions = np.argmax(model.predict(X_val),axis=-1)\n",
    "        predictions_training = np.argmax(model.predict(X_train),axis=-1)\n",
    "        confusion_training = tf.math.confusion_matrix(labels=Ytrain, predictions=predictions_training)\n",
    "        confusion = tf.math.confusion_matrix(labels=Yval, predictions=predictions)\n",
    "\n",
    "        # PRECISION = correct predictions of a class / total predictions for that class\n",
    "        zero_precision  = (np.sum(confusion[0,0])/np.sum(confusion[:,0])) * 100\n",
    "        one_precision   = (np.sum(confusion[1,1])/np.sum(confusion[:,1])) * 100\n",
    "\n",
    "        # Number of times network predicts a given class\n",
    "        zero_predictions  = (np.shape(np.where(predictions==0))[1]/predictions.shape[0])* 100\n",
    "        one_predictions   = (np.shape(np.where(predictions==1))[1]/predictions.shape[0])* 100\n",
    "\n",
    "        print('Zero prediction accuracy: '+str(zero_precision)[:2]+'%')\n",
    "        print('Zero: '+str(zero_predictions)[:3]+'% of predictions')\n",
    "        print('One prediction accuracy: '+str(one_precision)[:2]+'%')\n",
    "        print('One: '+str(one_predictions)[:3]+'% of predictions')\n",
    "\n",
    "        #print('Validation Loss at Best Epoch: '+str(es_callback.best*1))#+'%')\n",
    "\n",
    "        # ----- END NETWORK LOOP -----\n",
    "\n",
    "    NETWORK_SEED=0\n",
    "    model, LOSS = make_model() \n",
    "\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "    #---Testing Calcs \n",
    "    \n",
    "    running_window_yr = 10 #running mean\n",
    "    days_per_annualszn = 30+31+31+28\n",
    "    running_window = running_window_yr * days_per_annualszn\n",
    "\n",
    "    array_size = 100 - running_window_yr#-2   #there are less days in Cpval_pred and Ctval_true because of the week 3-4 averaging \n",
    "    #accuracy_test = np.zeros([len(RANDOM_SEED),array_size])\n",
    "    accuracy_test = np.zeros([array_size])\n",
    "    timeplot = np.arange(1850,1950)\n",
    "    timeplot = timeplot[running_window_yr:(len(timeplot))]\n",
    "    \n",
    "    fig, (ax1) = plt.subplots(1, figsize=(12,6))\n",
    "    \n",
    "    stored_accuracy = np.zeros(len(RANDOM_SEED))\n",
    "    ind = 0\n",
    "    ind2 = 0\n",
    "    \n",
    "    for SEED in RANDOM_SEED:\n",
    "        if ind < num_experiments:\n",
    "            model.load_weights(ddir_out+EXPERIMENT[-3:]+'_operationalseed'+str(SEED)+'.h5')\n",
    "            #---Testing Calcs \n",
    "            Ptest = model.predict(X_test_calcs)    #predicted values on validation - softmax output of confidence \n",
    "            Cptest_pred = Ptest.argmax(axis=1)     #0,1 of predicted  \n",
    "            Cttest_true = hotlabels_test_calcs.argmax(axis=1) #true values on validation \n",
    "\n",
    "            X_test_calcs = Xtest.stack(z=('lat','lon'))\n",
    "            hotlabels_test_calcs = onehotlabels_test[:,:model.output_shape[-1]]\n",
    "            results_test = model.evaluate(X_test_calcs,hotlabels_test_calcs,verbose = 2)\n",
    "\n",
    "            stored_accuracy[ind] = results_test[2] #2 hardcoded to get the accuracy (categorical and prediction accuracy are the same here)\n",
    "\n",
    "            for i in np.arange(0,len(Cptest_pred)-running_window_yr,(days_per_annualszn)):\n",
    "                if (running_window+i) <= (len(Cptest_pred)):\n",
    "                    modelcorr_test = Cptest_pred[i:running_window+i]==Cttest_true[i:running_window+i]\n",
    "                    nmodelcorr_test = modelcorr_test[modelcorr_test].shape[0]\n",
    "                    ntest_test = Cttest_true[i:running_window+i].shape[0]\n",
    "                    index_test = int(i/days_per_annualszn)\n",
    "                    accuracy_test[index_test] = 100*nmodelcorr_test/ntest_test\n",
    "        ind = ind+1\n",
    "            \n",
    "        accuracy_test_xr = xr.DataArray(accuracy_test)\n",
    "        filename = 'accuracy_testdata_'+str(running_window_yr)+'yr_runavg_exp'+EXPERIMENT[-3:]+'_seed'+str(SEED)+'.nc'\n",
    "        accuracy_test_xr.to_netcdf(ddir_out+filename, mode='w',format='NETCDF4')\n",
    "        #ax1.plot(timeplot, accuracy_test[ind2,:], label='Seed '+str(SEED))\n",
    "        ind2 = ind2+1\n",
    "\n",
    "        stored_accuracy_xr = xr.DataArray(stored_accuracy)\n",
    "        filename2 = 'overall_accuracy_testdata'+'_exp'+EXPERIMENT[-3:]+'_allseeds'+'.nc'\n",
    "        stored_accuracy_xr.to_netcdf(ddir_out+filename2, mode='w',format='NETCDF4')\n",
    "        \n",
    "        \n",
    "    # ax1.set(ylabel='Accuracy (\\%)')\n",
    "    # ax1.set(xlim=(timeplot[0],timeplot[-1]+1))\n",
    "    # ax1.set(xticks=(np.arange(timeplot[0],timeplot[-1]+5,step=5)))\n",
    "    # ax1.set(ylim=(40,70))\n",
    "    # ax1.set(yticks=(np.arange(40,75, step=5)))\n",
    "    # #ax1.set(ylim=(min(accuracy_test)-2,max(accuracy_val)+2))\n",
    "    # #ax1.set(yticks=(np.arange(np.round(min(accuracy_test)-2),max(accuracy_val)+4, step=5)))\n",
    "    # ax1.set(title='Experiment '+EXPERIMENT[-3:]+(str(running_window_yr)+'-yr Running Average of Accuracy'))\n",
    "    # ax1.legend(loc='upper left',ncol=2)\n",
    "    #plt.savefig(ddir_out+'accuracy_testdata_timeseries_'+str(running_window_yr)+'yr_runavg_exp'+EXPERIMENT[-3:]+'_allseeds.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
