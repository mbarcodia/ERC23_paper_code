{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc5b195-0fd8-48ce-9e6c-5f097940d67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from settings.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import settings\n",
    "from netCDF4 import Dataset\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab11f38c-768f-4017-809b-2a02e7837550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 15:14:21.056325: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: Marybeth Arcodia \n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.polynomial import polynomial\n",
    "import import_ipynb\n",
    "import sys\n",
    "import os \n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#%% Filter\n",
    "########################################\n",
    "#interp2.5_hist_1850-1949_PRECT_0.nc\n",
    "########################################\n",
    "NLABEL = 2\n",
    "\n",
    "YEARS = '1850-1949'\n",
    "STRT = pd.to_datetime('11-01-1850')\n",
    "END   = pd.to_datetime('2-28-1949')  + dt.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07245cce-92b5-4ffe-a159-c433f2ec3811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 100\n",
      "[0 1 2 3 4 5 6 7 8]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/core/dataarray.py:798\u001b[0m, in \u001b[0;36mDataArray._getitem_coord\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 798\u001b[0m     var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'time.dayofyear'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m#climo_full = []\u001b[39;00m\n\u001b[1;32m    107\u001b[0m anom \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label,ens_group \u001b[38;5;129;01min\u001b[39;00m \u001b[43mens_stacked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime.dayofyear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    110\u001b[0m     Xgroup \u001b[38;5;241m=\u001b[39m Xstacked\u001b[38;5;241m.\u001b[39mwhere(temp \u001b[38;5;241m==\u001b[39m label, drop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)     \u001b[38;5;66;03m#group all Jan 1s together, fit curve \u001b[39;00m\n\u001b[1;32m    112\u001b[0m     curve \u001b[38;5;241m=\u001b[39m polynomial\u001b[38;5;241m.\u001b[39mpolyfit(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,ens_group\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]),ens_group,\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#fit a line to all Jan 1s and not full timeseries\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/core/dataarray.py:6057\u001b[0m, in \u001b[0;36mDataArray.groupby\u001b[0;34m(self, group, squeeze, restore_coord_dims)\u001b[0m\n\u001b[1;32m   6052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(squeeze, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m   6053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   6054\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`squeeze` must be True or False, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msqueeze\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was supplied\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6055\u001b[0m     )\n\u001b[0;32m-> 6057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataArrayGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6058\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_coord_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrestore_coord_dims\u001b[49m\n\u001b[1;32m   6059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/core/groupby.py:367\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, group, squeeze, grouper, bins, restore_coord_dims, cut_kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hashable(group):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`group` must be an xarray.DataArray or the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname of an xarray variable or dimension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m group \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(group) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/core/dataarray.py:807\u001b[0m, in \u001b[0;36mDataArray.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m: T_DataArray, key: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T_DataArray:\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 807\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_coord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    809\u001b[0m         \u001b[38;5;66;03m# xarray-style array indexing\u001b[39;00m\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misel(indexers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_item_key_to_dict(key))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/core/dataarray.py:801\u001b[0m, in \u001b[0;36mDataArray._getitem_coord\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    800\u001b[0m     dim_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m--> 801\u001b[0m     _, key, var \u001b[38;5;241m=\u001b[39m \u001b[43m_get_virtual_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replace_maybe_drop_dims(var, name\u001b[38;5;241m=\u001b[39mkey)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/core/dataset.py:182\u001b[0m, in \u001b[0;36m_get_virtual_variable\u001b[0;34m(variables, key, dim_sizes)\u001b[0m\n\u001b[1;32m    179\u001b[0m ref_var \u001b[38;5;241m=\u001b[39m variables[ref_name]\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _contains_datetime_like_objects(ref_var):\n\u001b[0;32m--> 182\u001b[0m     ref_var \u001b[38;5;241m=\u001b[39m \u001b[43mDataArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(ref_var\u001b[38;5;241m.\u001b[39mdt, var_name)\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/core/dataarray.py:413\u001b[0m, in \u001b[0;36mDataArray.__init__\u001b[0;34m(self, data, coords, dims, name, attrs, indexes, fastpath)\u001b[0m\n\u001b[1;32m    411\u001b[0m data \u001b[38;5;241m=\u001b[39m _check_data_shape(data, coords, dims)\n\u001b[1;32m    412\u001b[0m data \u001b[38;5;241m=\u001b[39m as_compatible_data(data)\n\u001b[0;32m--> 413\u001b[0m coords, dims \u001b[38;5;241m=\u001b[39m \u001b[43m_infer_coords_and_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m variable \u001b[38;5;241m=\u001b[39m Variable(dims, data, attrs, fastpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    415\u001b[0m indexes, coords \u001b[38;5;241m=\u001b[39m _create_indexes_from_coords(coords)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/core/dataarray.py:146\u001b[0m, in \u001b[0;36m_infer_coords_and_dims\u001b[0;34m(shape, coords, dims)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m coords \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dim, coord \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dims, coords):\n\u001b[0;32m--> 146\u001b[0m         var \u001b[38;5;241m=\u001b[39m \u001b[43mas_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoord\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m         var\u001b[38;5;241m.\u001b[39mdims \u001b[38;5;241m=\u001b[39m (dim,)\n\u001b[1;32m    148\u001b[0m         new_coords[dim] \u001b[38;5;241m=\u001b[39m var\u001b[38;5;241m.\u001b[39mto_index_variable()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/core/variable.py:164\u001b[0m, in \u001b[0;36mas_variable\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MissingDimensionsError(\n\u001b[1;32m    160\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has more than 1-dimension and the same name as one of its \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimensions \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m. xarray disallows such variables because they \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconflict with the coordinates used to label dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         )\n\u001b[0;32m--> 164\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_index_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/core/variable.py:2938\u001b[0m, in \u001b[0;36mIndexVariable.to_index_variable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2936\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_index_variable\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m IndexVariable:\n\u001b[1;32m   2937\u001b[0m     \u001b[38;5;124;03m\"\"\"Return this variable as an xarray.IndexVariable\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/core/variable.py:2906\u001b[0m, in \u001b[0;36mIndexVariable.copy\u001b[0;34m(self, deep, data)\u001b[0m\n\u001b[1;32m   2882\u001b[0m \u001b[38;5;124;03m\"\"\"Returns a copy of this object.\u001b[39;00m\n\u001b[1;32m   2883\u001b[0m \n\u001b[1;32m   2884\u001b[0m \u001b[38;5;124;03m`deep` is ignored since data is stored in the form of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2903\u001b[0m \u001b[38;5;124;03m    data copied from original.\u001b[39;00m\n\u001b[1;32m   2904\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2906\u001b[0m     ndata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2908\u001b[0m     ndata \u001b[38;5;241m=\u001b[39m as_compatible_data(data)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/core/indexing.py:1503\u001b[0m, in \u001b[0;36mPandasIndexingAdapter.copy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, deep: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PandasIndexingAdapter:\n\u001b[1;32m   1495\u001b[0m     \u001b[38;5;66;03m# Not the same as just writing `self.array.copy(deep=deep)`, as\u001b[39;00m\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;66;03m# shallow copies of the underlying numpy.ndarrays become deep ones\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;66;03m# >>> len(pickle.dumps((self.array, self.array.copy(deep=False))))\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m     \u001b[38;5;66;03m# 8000341\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\n\u001b[0;32m-> 1503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/core/indexing.py:1418\u001b[0m, in \u001b[0;36mPandasIndexingAdapter.__init__\u001b[0;34m(self, array, dtype)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, array: pd\u001b[38;5;241m.\u001b[39mIndex, dtype: DTypeLike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1418\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m \u001b[43msafe_cast_to_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype \u001b[38;5;241m=\u001b[39m get_valid_numpy_dtype(array)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/core/utils.py:140\u001b[0m, in \u001b[0;36msafe_cast_to_index\u001b[0;34m(array)\u001b[0m\n\u001b[1;32m    138\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    139\u001b[0m     index \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mIndex(np\u001b[38;5;241m.\u001b[39masarray(array), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_maybe_cast_to_cftimeindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/core/utils.py:70\u001b[0m, in \u001b[0;36m_maybe_cast_to_cftimeindex\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(index) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCFTimeIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m index\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/coding/cftimeindex.py:315\u001b[0m, in \u001b[0;36mCFTimeIndex.__new__\u001b[0;34m(cls, data, name, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 315\u001b[0m     \u001b[43massert_all_valid_date_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    317\u001b[0m         name \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/coding/cftimeindex.py:230\u001b[0m, in \u001b[0;36massert_all_valid_date_type\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sample, cftime\u001b[38;5;241m.\u001b[39mdatetime):\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCFTimeIndex requires cftime.datetime \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects. Got object of \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(date_type)\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCFTimeIndex requires using datetime \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects of all the same type.  Got\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(data)\n\u001b[1;32m    234\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/xarray/coding/cftimeindex.py:230\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sample, cftime\u001b[38;5;241m.\u001b[39mdatetime):\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCFTimeIndex requires cftime.datetime \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects. Got object of \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(date_type)\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28;43misinstance\u001b[39;49m(value, date_type) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m data):\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCFTimeIndex requires using datetime \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects of all the same type.  Got\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(data)\n\u001b[1;32m    234\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#-- Pull Parameters from settings.ipynb ------\n",
    "exp_num = 1\n",
    "\n",
    "for sub_exp in np.arange(exp_num*100,exp_num*100+10):\n",
    "    FOLDER     = 'exp_'+str(exp_num)\n",
    "    EXPERIMENT = 'exp_'+str(exp_num)+'/exp_'+str(sub_exp)\n",
    "    \n",
    "    print(\"Experiment \"+str(sub_exp))\n",
    "          \n",
    "    ddir_in = DIRECTORY_IN # Need to name\n",
    "    ddir_out = DIRECTORY_OUT #need to name \n",
    "    params = settings.get_settings(EXPERIMENT)\n",
    "\n",
    "    PREDICTOR_VAR  = params['PREDICTOR_VAR']         \n",
    "    PREDICTAND_VAR = params['PREDICTAND_VAR']              \n",
    "    REGION_TOR     = params['REGION_TOR']            \n",
    "    REGION_TAND    = params['REGION_TAND']            \n",
    "    training_ens   = params['training_ens']            \n",
    "    validation_ens = params['validation_ens']           \n",
    "    testing_ens    = params['testing_ens']           \n",
    "    train_list     = params['train_list']           \n",
    "    train_val_list = params['train_val_list']\n",
    "    lead           = params['lead']            \n",
    "    days_average   = params['days_average']            \n",
    "    GLOBAL_SEED    = params['GLOBAL_SEED']            \n",
    "    HIDDENS        = params['HIDDENS']          \n",
    "    DROPOUT        = params['DROPOUT']            \n",
    "    RIDGE1         = params['RIDGE1']                    \n",
    "    LR_INIT        = params['LR_INIT']\n",
    "    BATCH_SIZE     = params['BATCH_SIZE']           \n",
    "    RANDOM_SEED    = params['RANDOM_SEED']            \n",
    "    act_fun        = params['act_fun']            \n",
    "    N_EPOCHS       = params['N_EPOCHS']           \n",
    "    PATIENCE       = params['PATIENCE']   \n",
    "    \n",
    "#>>>>>SET UP <<<<<<<<<<<<<<<\n",
    "    np.random.seed(GLOBAL_SEED)\n",
    "    random.seed(GLOBAL_SEED)\n",
    "    tf.compat.v1.random.set_random_seed(GLOBAL_SEED)\n",
    "\n",
    "    ens_calc_members = np.hstack((np.array(training_ens),np.array(validation_ens)))\n",
    "    anom_calc_members = np.hstack((np.array(ens_calc_members),np.array(testing_ens)))\n",
    "    print(ens_calc_members)\n",
    "    print(anom_calc_members)\n",
    "    \n",
    "    count = 0\n",
    "    for i in ens_calc_members:\n",
    "        infile = 'interp2.5_LE2-'+str(i)+'.'+PREDICTOR_VAR+'.'+REGION_TOR+'.'+YEARS+'.nc'       \n",
    "        X = xr.open_dataset(ddir_in+infile)[PREDICTOR_VAR]   #reads in just the PRECT var without the time_bnds dimension \n",
    "        X_nptime = np.array(X.time)                 #for some annoying reason, it needed to be converted to numpy for creating DataArray   \n",
    "        X_nplat = np.array(X.lat)\n",
    "        X_nplon = np.array(X.lon)\n",
    "        #print(infile)\n",
    "        del infile \n",
    "        \n",
    "        if count == 0: # don't rewrite empty matrix each time \n",
    "            X_all = xr.DataArray(np.zeros((len(train_val_list),X.shape[0],X.shape[1],X.shape[2]))+np.nan,\n",
    "                                 dims = ['ens','time','lat','lon'],\n",
    "                                 coords = [('ens',np.arange(len(train_val_list))+1),('time', X_nptime),('lat',X_nplat),('lon',X_nplon)])\n",
    "        if PREDICTOR_VAR == 'PRECT':\n",
    "            X_convert = X * 1000. * 86400. # m/s * 1000mm/m * 86400s/day\n",
    "            X_all[count,:,:,:] = X_convert\n",
    "        else:\n",
    "             X_all[count,:,:,:] = X   \n",
    "        count = count+1\n",
    "        del X #, X_convert \n",
    "        \n",
    "    #Take mean over ensembles & save\n",
    "    X_all_mean = X_all.mean('ens',skipna=True)\n",
    "    X_all_mean.to_netcdf(ddir_out+'ensmean-'+train_val_list+'_'+REGION_TOR+'_'+YEARS+'_'+PREDICTOR_VAR+'.nc')\n",
    "    \n",
    "### Calculate Anomalies by Subtracting Ensemble Mean\n",
    "    del X_nptime, X_nplat, X_nplon, X_all\n",
    "    del count, i \n",
    "    \n",
    "    ensmean = X_all_mean#xr.open_dataset(ddir_out+'ensmean-'+train_val_list+'_'+REGION_TOR+'_'+YEARS+'_'+PREDICTOR_VAR+'.nc')['__xarray_dataarray_variable__']\n",
    "    ens_stacked = ensmean.stack(z=('lat','lon'))\n",
    "    \n",
    "    del X_all_mean\n",
    "\n",
    "    #To be able to save all anomaly data in 1 file for training\n",
    "    X_nptime = np.array(ensmean.time)                 #for some annoying reason, it needed to be converted to numpy for creating DataArray   \n",
    "    X_nplat = np.array(ensmean.lat)\n",
    "    X_nplon = np.array(ensmean.lon)\n",
    "    anom_concat = xr.DataArray(np.zeros((len(training_ens),ensmean.shape[0],ensmean.shape[1],ensmean.shape[2]))+np.nan,\n",
    "                                dims = ['ens','time','lat','lon'],\n",
    "                                coords = [('ens',np.arange(len(training_ens))),('time', X_nptime),('lat',X_nplat),('lon',X_nplon)])\n",
    "    del X_nptime, X_nplat, X_nplon\n",
    "\n",
    "#Calculate Daily Anomaly by subtracting the climatology from each grid point, for each day \n",
    "\n",
    "    count = 0 \n",
    "    for i in anom_calc_members:\n",
    "        infile = 'interp2.5_LE2-'+str(i)+'.'+PREDICTOR_VAR+'.'+REGION_TOR+'.'+YEARS+'.nc'     \n",
    "\n",
    "        outfile_singles = PREDICTOR_VAR+'_'+REGION_TOR+'_'+YEARS+'_ens'+str(i)+'_dailyanom_detrend.nc'\n",
    "\n",
    "        X = xr.open_dataset(ddir_in+infile)[PREDICTOR_VAR]\n",
    "        if PREDICTOR_VAR == 'PRECT':\n",
    "            X = X * 1000. * 86400. # m/s * 1000mm/m * 86400s/day = [mm/day]\n",
    "\n",
    "        Xstacked = X.stack(z=('lat', 'lon'))\n",
    "        del X\n",
    "        \n",
    "        temp = Xstacked['time.dayofyear']\n",
    "        #climo_full = []\n",
    "        anom = []\n",
    "        for label,ens_group in ens_stacked.groupby('time.dayofyear'):\n",
    "\n",
    "            Xgroup = Xstacked.where(temp == label, drop = True)     #group all Jan 1s together, fit curve \n",
    "\n",
    "            curve = polynomial.polyfit(np.arange(0,ens_group.shape[0]),ens_group,1) #fit a line to all Jan 1s and not full timeseries\n",
    "            trend = polynomial.polyval(np.arange(0,ens_group.shape[0]),curve,tensor=True)  #don't assume trend is the same throughout seasonal cycle\n",
    "            trend = np.swapaxes(trend,0,1)\n",
    "            diff  = Xgroup - trend \n",
    "            anom.append(diff)\n",
    "\n",
    "        anom_xr_trend = xr.concat(anom,dim='time').unstack()\n",
    "        anom_xr_trend = anom_xr_trend.sortby('time')\n",
    "\n",
    "        X_nptime = np.array(anom_xr_trend.time)                 #need to convert to numpy for creating DataArray   \n",
    "        X_nplat = np.array(anom_xr_trend.lat)\n",
    "        X_nplon = np.array(anom_xr_trend.lon)\n",
    "\n",
    "        detrend_anom_4write = xr.DataArray(np.zeros((anom_xr_trend.shape[0],anom_xr_trend.shape[1],anom_xr_trend.shape[2]))+np.nan,\n",
    "                     dims = ['time','lat','lon'],\n",
    "                     coords = [('time', X_nptime),('lat',X_nplat),('lon',X_nplon)]) \n",
    "        detrend_anom_4write[:,:,:] = anom_xr_trend[:,:,:]\n",
    "        detrend_anom_4write.to_netcdf(ddir_out+outfile_singles)\n",
    "\n",
    "        if count < len(training_ens):\n",
    "            anom_concat[count,:,:,:] = anom_xr_trend   \n",
    "            count = count+1\n",
    "\n",
    "    anom_concat = anom_concat[:len(training_ens),:,:,:]\n",
    "    outfile_concat = PREDICTOR_VAR+'_'+REGION_TOR+'_'+YEARS+'_'+'ens'+train_list+'_dailyanom_detrend.nc'\n",
    "    anom_concat.to_netcdf(ddir_out+outfile_concat)\n",
    "    \n",
    "    del outfile_concat, anom, diff, trend, curve, Xgroup, temp, Xstacked, detrend_anom_4write\n",
    "\n",
    "\n",
    "    # Calculate Running Averages of Anomalies for Prediction problems\n",
    "    #### Only Needed for Predictand Variable\n",
    "\n",
    "    predictand_var = anom_concat #xr.open_dataset(ddir_out+VAR+'_'+REGION+'_'+YEARS+'_'+'ens'+train_list+'_dailyanom_detrend.nc')['__xarray_dataarray_variable__']\n",
    "    del anom_concat\n",
    "\n",
    "    #Need to spatially average over the region we want to predict \n",
    "\n",
    "    lat_avg = np.mean(predictand_var,axis=2)\n",
    "    box_avg = np.mean(lat_avg,axis=2)\n",
    "    del predictand_var\n",
    "    \n",
    "#    Check that data has 50% above median\n",
    "    for ENS in range(0,len(training_ens)): \n",
    "        ind = (copy_box_avg[ENS,:]<=np.median(box_avg[ENS,:]))\n",
    "        copy_box_avg[ENS,ind] = np.median(box_avg[ENS,:])\n",
    "        ind = (copy_box_avg[ENS,:]>np.median(box_avg[ENS,:]))\n",
    "        copy_box_avg[ENS,ind] = 1.0\n",
    "\n",
    "        num_above_median = np.count_nonzero(copy_box_avg[ENS,:] == 1)\n",
    "        perc_above_median = np.round((num_above_median/len(predictand_var.time))*100,2)\n",
    "        print('Ensemble '+str(ENS)+ ' has '+ str(perc_above_median) + '% above the median of '+str(np.round(np.median(box_avg[ENS,:]),3)))\n",
    "\n",
    "    #Calculate Forward Running 14-Day Average to make precip less noisy \n",
    "    #Resulting Timeseries is len(timeseries minus (days_average-1) because can't take forward average of first days\n",
    "    #NN can't handle NaNs but if I need the timeseries to be full length, then remove .dropna code \n",
    "\n",
    "    box_time_np = np.array(box_avg.time)\n",
    "    box_time_np = box_time_np[:len(box_avg.time)]\n",
    "    \n",
    "    weekly_run_avg = xr.DataArray(np.zeros((len(training_ens),box_avg.shape[1])),\n",
    "                                dims = ['ens','time'],\n",
    "                                coords = [('ens',np.arange(len(training_ens))),('time',box_time_np)])\n",
    "    for ENS in range(0,len(training_ens)):\n",
    "        weekly_run_avg[ENS,:] = box_avg[ENS,:].rolling(time=days_average, center=False).mean()     #.dropna(dim=\"time\")\n",
    "\n",
    "    weekly_run_avg = weekly_run_avg.dropna(dim=\"time\")  #need to add in .dropna here because indexing was difficult in previous line \n",
    "\n",
    "    del box_avg, lat_avg, box_time_np\n",
    "    \n",
    "#Check to see if averaging precip makes distribution more gaussian\n",
    "# This check is important to keep just to make sure data is close to ~50%\n",
    "\n",
    "    copy_week_avg = np.copy(weekly_run_avg)\n",
    "\n",
    "    print(\"Check distribution about 0 for \"+str(days_average)+\"-day average\")\n",
    "    for ENS in range(0,len(training_ens)): \n",
    "        ind = (copy_week_avg[ENS,:]<=0)\n",
    "        copy_week_avg[ENS,ind] = 0.0\n",
    "        ind = (copy_week_avg[ENS,:]>0)\n",
    "        copy_week_avg[ENS,ind] = 1.0\n",
    "\n",
    "        num_above_zero = np.count_nonzero(copy_week_avg[ENS,:] == 1)\n",
    "        #print(num_above_zero)\n",
    "        perc_above_zero = np.round((num_above_zero/len(weekly_run_avg.time))*100,2)\n",
    "        print('Ensemble '+str(ENS)+ ' has '+ str(perc_above_zero) + '% above 0.0 mm/day')\n",
    "\n",
    "    outfile_concat = PREDICTAND_VAR+'_'+REGION_TAND+'_boxavg_'+YEARS+'_'+'ens'+train_list+'_dailyanom_detrend_'+str(days_average)+'dayavg.nc'\n",
    "    weekly_run_avg.to_netcdf(ddir_out+outfile_concat)\n",
    "    \n",
    "    del weekly_run_avg, outfile_concat, copy_week_avg, ind, num_above_zero, perc_above_zero\n",
    "\n",
    "\n",
    "    ## Compute Weekly Avg for Validation Ensemble Member\n",
    "\n",
    "    predictand_var = xr.open_dataset(ddir_out+PREDICTAND_VAR+'_'+REGION_TAND+'_'+YEARS+'_ens'+str(validation_ens)+'_dailyanom_detrend.nc')['__xarray_dataarray_variable__']\n",
    "\n",
    "    #Need to spatially average over the region we want to predict \n",
    "\n",
    "    lat_avg = np.mean(predictand_var,axis=1)\n",
    "    box_avg = np.mean(lat_avg,axis=1)\n",
    "    del predictand_var\n",
    "    \n",
    "    copy_box_avg = np.copy(box_avg)\n",
    "    copy_box_avg = np.array(copy_box_avg)\n",
    "\n",
    "    box_time_np = np.array(box_avg.time)\n",
    "    box_time_np = box_time_np[:len(box_avg.time)]\n",
    "\n",
    "    weekly_run_avg = xr.DataArray((np.zeros(box_avg.shape[0])),\n",
    "                                dims = ['time'],\n",
    "                                coords = [('time',box_time_np)])\n",
    "    weekly_run_avg = box_avg.rolling(time=days_average, center=False).mean()     #.dropna(dim=\"time\")\n",
    "\n",
    "    weekly_run_avg = weekly_run_avg.dropna(dim=\"time\")  #need to add in .dropna here because indexing was difficult in previous line \n",
    "\n",
    "    outfile_concat = PREDICTAND_VAR+'_'+REGION_TAND+'_boxavg_'+YEARS+'_'+'ens'+str(validation_ens)+'_dailyanom_detrend_'+str(days_average)+'dayavg.nc'\n",
    "    weekly_run_avg.to_netcdf(ddir_out+outfile_concat)\n",
    "    \n",
    "    del copy_box_avg, box_time_np, weekly_run_avg, outfile_concat\n",
    "\n",
    "\n",
    "    ## Compute Weekly Avg for TESTING Ensemble Member\n",
    "\n",
    "    predictand_var = xr.open_dataset(ddir_out+PREDICTAND_VAR+'_'+REGION_TAND+'_'+YEARS+'_ens'+str(testing_ens)+'_dailyanom_detrend.nc')['__xarray_dataarray_variable__']\n",
    "    # predictand_var\n",
    "    # plt.plot(predictand_var[:,0,0])\n",
    "    # print(np.mean(predictand_var[:,0,0]))\n",
    "\n",
    "    #Need to spatially average over the region we want to predict \n",
    "\n",
    "    lat_avg = np.mean(predictand_var,axis=1)\n",
    "    box_avg = np.mean(lat_avg,axis=1)\n",
    "    del predictand_var\n",
    "    \n",
    "    copy_box_avg = np.copy(box_avg)\n",
    "    copy_box_avg = np.array(copy_box_avg)\n",
    "\n",
    "    box_time_np = np.array(box_avg.time)\n",
    "    box_time_np = box_time_np[:len(box_avg.time)]\n",
    "\n",
    "    weekly_run_avg = xr.DataArray((np.zeros(box_avg.shape[0])),\n",
    "                                dims = ['time'],\n",
    "                                coords = [('time',box_time_np)])\n",
    "    weekly_run_avg = box_avg.rolling(time=days_average, center=False).mean()     #.dropna(dim=\"time\")\n",
    "\n",
    "    weekly_run_avg = weekly_run_avg.dropna(dim=\"time\")  #need to add in .dropna here because indexing was difficult in previous line \n",
    "\n",
    "    outfile_concat = PREDICTAND_VAR+'_'+REGION_TAND+'_boxavg_'+YEARS+'_'+'ens'+str(testing_ens)+'_dailyanom_detrend_'+str(days_average)+'dayavg.nc'\n",
    "    weekly_run_avg.to_netcdf(ddir_out+outfile_concat)\n",
    "    \n",
    "    del copy_box_avg, box_time_np, weekly_run_avg, outfile_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a96fb22-8023-4201-bfa1-f087995398c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
